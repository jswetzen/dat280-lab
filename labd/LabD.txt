Distributed Erlangand Map-..ReduceThegoalofthislabistomakethena•vemap-..reduceimplementationpresentedinthelecture,alittlelessna•ve. Specifically,wewill makeitrunonmultipleErlangnodes,balancetheload between them,andbegintomake the code fault-..tolerant.ErlanresourcesYouwillprobablyneedtoconsulttheErlangdocumentationduringthisexercise.Youcanfindthecompletedocumentationhere:http://www.erlang.org/doc/Tofinddocumentationofaparticularmodule,usethelistofmoduleshere:http://www.erlang.org/doc/man_index.htmlNotethattheWindowsinstalleralsoinstallsthedocumentationlocally,soif youareusingWindowsthenyoucanjustopenthedocumentationviaalinkintheStartmenu.Connecting multipleErlang nodesThefirststepistosetupnetworkofconnectedErlangnodestoplaywith.Thiscanbedonein two ways:RunningmultipleErlangnodesononemachineStartseveralterminalwindows/Windowscmdwindows,andineachone startnamedErlangshell.Dothisusingacommandsuchaserl Ðsname foo oLinuxortheMac,andwerl Ðsname foo oWindows. (TheWindowsversionstartsErlanginitsownwindow,withsomeuseful menus). Thepromptdisplayed bytheErlangshellwillshowyou whateach Erlangnodeyou created iscalled.Forexample,onmymachine the promptis(baz@JohnsTablet2012)1> ThistellsmethatthenodeIcreatediscalledbaz@JohnsTablet2012(anErlangatom).RunningErlangnodesonmultiplemachinesItÕsmorefunusingseveral machines.Theprocedureisthesameasabove,butfirstyoumustensure that allmachinesusethesamecookie.Editthefile.erlang.cookieinyourhomedirectoryoneachmachine,andplacethesameErlangatom ineachone.ThenstartErlangnodesasabove;aslongasthemachinesareonthesamenetwork,thentheyshouldbeabletofindeachother.Inparticular,machinesinthelabsatChalmersoughttobeabletofindeachother.ConnectingthenodestogetherYourErlangnodesarenotyetconnectedÉcallingnodes()oanyofthemwillreturn theemptylist. Toconnectthem,callnet_adm:ping(NodeB). oNodeA (two ofyournodenames).Theresultshould bepong,andcallingnodes()afterwardsoeithernodeshould showyou theother.Connectallyournodesin thisway.NotethatbecauseErlangbuildscompletenetwork,thenyouneedonlyconnecteachnodetooneother nodeyourself.Help!ItdoesnÕtwork¥ Onmultiplemachines,checkthatthecookiereallyisthesameonallthenodes.Call erlang:get_cookie()oeach nodeto makesure. ¥ IfNodeAcanÕtconnecttoNodeB,tryconnectingNodeBtoNodeA.Sometimesthathelps!¥ Perhapsone ormore ofyourmachinesrequiresloginbefore the networkconnectioncanbeestablished.In aWindowsnetwork,tryvisitingtheShared Folderoeach machinefromtheothersÑthismayprompt for apassword,andonceyougivethepasswordthenErlangwillalsobeabletoconnect.Remote Procedure CallsWeÕllstartbymakingremoteprocedurecallstoothernodes.WeÕllcallio:format,whichisErlangÕsversionofprintf.Tryrpc:call(OtherNode,io,format,[ÒhelloÓ]). YouwillfindÒhelloÓprintedonyourownnode!Erlangredirectstheoutputofprocessesspawned onothernodesbackto theoriginalspawningnodeÑsoio:formatreallydidrunontheothernode,butitsoutputwasreturnedtothefirstone. Toforceoutputothenodewhereio:formatruns,wealso supplyanexplicitdestinationfortheoutput.Tryrpc:call(OtherNode,io,format,[user,ÓhelloÓ,[]]). (wherethelast argument isthelist of valuesfor escapeslike~pinthestringÉsinceÒhelloÓcontainsnescapes,then wepasstheemptylist).Makesurethattheoutputreallydoesappearothecorrectnode.Compiling and loadingLoading codeonothernodesisverysimple. Writeasimplemodulecontainingthisfunction:-module(foo).-compile(export_all).foo() -> io:format(user,ÓhelloÓ,[]).Nowyoucancompilethismoduleintheshellviac(foo). andyoucanthenloaditontoallyournodesvia the commandnl(foo). Tryusingrpc:calltocallfoo:foooeach node,checkingthattheoutputappearsothecorrectnode.Na•veMap-..ReduceBelowyou willfind thesourcecodeofthreeofthemodulespresented in thelectureomap-..reduce:verysimple map-..reduceimplementation oonenode(both sequentialand parallel),and two clientsÑa webcrawlerandpage rankcalculator.Compile these modules,andensure thatyoucancrawlapartoftheweb.crawl(Òhttp://www.cse.chalmers.se/Ó,3). YouwillneedtostartErlangÕshttpclientfirst, usinginets:start().Thepagerankcalculatorusestheinformationcollectedbythewebcrawler,butitassumesthattheoutputoftheweb crawlerhasbeen saved in adetsfileÑa file thatcontainssetofkey-..valuepairs.Youwillneedtouse detstodothislab.Youcanfindthe documentationhere (http://www.erlang.org/doc/man/dets.htmlÐandthere isquite lotofitÑbutyou willonlyneed afewfunctionsfromthismodule.¥ dets:open_fileÑseecodebelowforusage¥ dets:insertÑwhichinsertsalistofkey-..valuepairsintothefile¥ dets:lookupÑwhichreturnsalistofallthekey-..valuepairswithagivenkeySave the resultsfromthe webcrawlerindetsfile calledweb.dat,andcheckthatthepagerankingalgorithmworks.Thencopyweb.datontoallyournodesÑthiswillenableyoutodistributethepagerankcomputationacrossyour network.Youshouldcollect 40-..100MBofwebdata sothatthe pagerankingalgorithmtakesappreciabletimetorun.DistributinMap-..ReduceModifytheparallelmap-..reduceimplementationsothat it spawnsworker processesonallof your nodes.Measuretheperformanceofthepage-..rankingalgorithmwiththeoriginalparallelversion,andyournewdistributedversion.Load-..balancing Map-..ReduceOfcourseitisnotreallysensibletospawnalltheworkerprocessesatthesametime.Instead,weshouldstartenoughworkers tokeepallthenodes busy,thensendeachnodenewworkas itcompletes its previous job.Writeaworkerpoolfunctionwhich,givenalist of 0-..aryfunctions,returnsalistoftheirresults,distributingthe workacrossthe connectednodesinthisway.Thatis,semanticallyworker_pool(Funs) -> [Fun() || Fun <-Funs],buttheimplementationshouldmakeuseofall thenodesinyournetwork. Agoodapproachistostartseveralworkerprocesses oneachnode,eachofwhichkeeps requestinganewfunctiontocall,thencallingitandreturningitsresultto themaster,untilnomoreworkremainsto bedone.Modifythemap-..reduceimplementationagaintomakeuseof your worker poolinboththemapandthereducephases.Measuretheperformanceofpagerankingwith yournewdistributed map-..reduceÉis itfaster?Fault-..tolerant Map-..ReduceEnhanceyourworker-..poolto monitorthestateoftheworkerprocesses,so thatifaworkershould die,then itsworkisreassigned to anewworker.TestyourfaulttolerancebykillingoneofyourErlangnodes(notthemaster)whilethepage-..rankingalgorithmisrunning.It shouldcomplete,withthesameresults,despitethefailure.Hand insYoushouldsubmitthecodeofthethreeversionsofmap-..reducedescribedabove,together withyour performancemeasurements.Describeyourset-..up:wereyou runningoonemachineorseveral,howmuch web datawereyou searching?Whatconclusionswould you drawfromthisexercise?ThedeadlineismidnightonFriday,23rdMay.Morefullmap-..reduceimplementationdoeslotmore thanthis,ofcourse.The nextstepwouldbe toavoidsendingallthe data via the masterÑtheresultsof eachmapper shouldbesent directlytotheright reducerÉalthoughthisintroducesalot morecomplexity.Somethingtoexperiment withlater,perhaps?TheCode%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % %% This is a very simple implementation of map-reduce, in both %% sequential and parallel versions.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % -module(map_reduce).-compile(export_all). %% We begin with a simple sequential implementation, just to define%% the semantics of map-reduce. %% The input is a collection of key-value pairs. The map function maps%% each key value pair to a list of key-value pairs. The reduce%% function is then applied to each key and list of corresponding%% values, and generates in turn a list of key-value pairs. These are %% the result. map_reduce_seq(Map,Reduce,Input) ->Mapped = [{K2,V2}|| {K,V} <-Input,{K2,V2} <-Map(K,V)],reduce_seq(Reduce,Mapped). reduce_seq(Reduce,KVs) ->[KV || {K,Vs} <-group(lists:sort(KVs)),KV <-Reduce(K,Vs)]. group([]) ->[];group([{K,V}|Rest]) ->group(K,[V],Rest). group(K,Vs,[{K,V}|Rest]) ->group(K,[V|Vs],Rest);group(K,Vs,Rest) -> [{K,lists:reverse(Vs)}|group(Rest)]. map_reduce_par(Map,M,Reduce,R,Input) ->Parent = self(),Splits = split_into(M,Input),Mappers =[spawn_mapper(Parent,Map,R,Split)|| Split <-Splits],Mappeds =[receive {Pid,L} -> L end || Pid <-Mappers],Reducers = [spawn_reducer(Parent,Reduce,I,Mappeds)|| I <-lists:seq(0,R-1)],Reduceds = [receive {Pid,L} -> L end || Pid <-Reducers],lists:sort(lists:flatten(Reduceds)). spawn_mapper(Parent,Map,R,Split) ->spawn_link(fun() -> Mapped = [{erlang:phash2(K2,R),{K2,V2}}|| {K,V} <-Split,{K2,V2} <-Map(K,V)], Parent ! {self(),group(lists:sort(Mapped))}end). split_into(N,L) ->split_into(N,L,length(L)). split_into(1,L,_) ->[L];split_into(N,L,Len) ->{Pre,Suf} = lists:split(Len div N,L),[Pre|split_into(N-1,Suf,Len-(Len div N))]. spawn_reducer(Parent,Reduce,I,Mappeds) ->Inputs = [KV|| Mapped <-Mappeds,{J,KVs} <-Mapped,I==J,KV <-KVs],spawn_link(fun() -> Parent ! {self(),reduce_seq(Reduce,Inputs)} end).%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % %% This implements a page rank algorithm using map-reduce%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % -module(page_rank).-compile(export_all). %% Use map_reduce to count word occurrences map(Url,ok) ->[{Url,Body}] = dets:lookup(web,Url),Urls = web_crawler:find_urls(Url,Body),[{U,1} || U <-Urls]. reduce(Url,Ns) ->[{Url,lists:sum(Ns)}]. %% 188 seconds page_rank() ->dets:open_file(web,[{file,"web.dat"}]),Urls = dets:foldl(fun({K,_},Keys)->[K|Keys] end,[],web),map_reduce:map_reduce_seq(fun map/2, fun reduce/2, [{Url,ok} || Url <-Urls]). %% 86 seconds page_rank_par() ->dets:open_file(web,[{file,"web.dat"}]),Urls = dets:foldl(fun({K,_},Keys)->[K|Keys] end,[],web),map_reduce:map_reduce_par(fun map/2, 32, fun reduce/2, 32,[{Url,ok} || Url <-Urls]). %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % %% This module defines a simple web crawler using map-reduce.%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% % -module(crawl).-compile(export_all). %% Crawl from a URL, following links to depth D.%% Before calling this function, the inets service must%% be started using inets:start().crawl(Url,D) ->Pages = follow(D,[{Url,undefined}]),[{U,Body} || {U,Body} <-Pages,Body /= undefined]. follow(0,KVs) ->KVs;follow(D,KVs) ->follow(D-1,map_reduce:map_reduce_par(fun map/2, 20, fun reduce/2,1, KVs)). map(Url,undefined) ->Body = fetch_url(Url),[{Url,Body}] ++[{U,undefined} || U <-find_urls(Url,Body)];map(Url,Body) ->[{Url,Body}]. reduce(Url,Bodies) ->case [B || B <-Bodies, B/=undefined] of [] ->[{Url,undefined}];[Body] ->[{Url,Body}]end. fetch_url(Url) ->case httpc:request(Url) of{ok,{_,_Headers,Body}} -> Body;_ ->"" end. %% Find all the urls in an Html page with a given Url.find_urls(Url,Html) ->Lower = string:to_lower(Html),%% Find all the complete URLs that occur anywhere in the pageAbsolute = case re:run(Lower,"http://.*?(?=\")",[global]) of{match,Locs} ->[lists:sublist(Html,Pos+1,Len)|| [{Pos,Len}] <-Locs]; _ ->[]end,%% Find links to files in the same directory, which need to be%% turned into complete URLs.Relative = case re:run(Lower,"href *=*\"(?!http:).*?(?=\")",[global]) of{match,RLocs} ->[lists:sublist(Html,Pos+1,Len)|| [{Pos,Len}] <-RLocs]; _ ->[]end,Absolute ++ [Url++"/"++lists:dropwhile(fun(Char)->Char==$/ end,tl(lists:dropwhile(fun(Char)->Char/=$" end, R))) || R <-Relative]. 